# Preset MSKA-SLT con decoder mBART-large CC25 inspirado en el paper original.
preset:
  name: mska_paper_mbart
  description: >-
    Configuraci√≥n basada en MSKA-SLT que replica los 8 bloques atencionales por stream
    con 6 cabezas y activa un decoder mBART-large CC25 para inferencia offline.
  reference: docs/mska-paper-config.md

data:
  tokenizer: facebook/mbart-large-cc25
  max_target_length: 196

model:
  use_mska: true
  projector_dim: 256
  d_model: 256
  temporal_layers: 8
  temporal_nhead: 6
  temporal_dim_feedforward: 1024
  temporal_dropout: 0.1
  sequence_length: 128
  decoder_layers: 12
  decoder_heads: 16
  decoder_dropout: 0.3
  decoder_model: facebook/mbart-large-cc25
  decoder_kwargs:
    forced_bos_token_id: 250004
  mska_heads: 6
  mska_stream_heads: 6
  mska_temporal_blocks: 8
  mska_temporal_kernel: 3
  mska_temporal_dilation: 1
  mska_dropout: 0.1
  mska_translation_weight: 1.0
  mska_ctc_weight: 1.0
  mska_distillation_weight: 1.0
  mska_distillation_temperature: 1.0
  mska_gloss_hidden_dim: 512
  mska_gloss_second_hidden_dim: 256
  mska_gloss_dropout: 0.1
  mska_gloss_fusion: concat

training:
  epochs: 40

optim:
  optimizer: adamw
  lr: 1.0e-5
  weight_decay: 1.0e-3
  scheduler: cosine
  scheduler_step_size: 40
