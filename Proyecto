# SeñAR / MSKA — Implementación Multi‑stream (cara+manos+pose) inspirada en SignMusketeers
# v9 — Dataset, modelos, entrenamiento, evaluación y exportación
# Estructura por módulos en un solo archivo para facilitar copia/ajuste inmediato.
# NOTA: reemplaza rutas (G:/...) según tu entorno Windows.

import os
import math
import json
import random
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# =============================
# 1) Utilidades generales
# =============================

def set_seed(seed: int = 42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True


def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:
    # x: [..., T, D], mask: [..., T] con 1 válidos, 0 inválidos
    mask = mask.unsqueeze(-1).type_as(x)
    s = (x * mask).sum(dim=dim)
    n = mask.sum(dim=dim).clamp(min=1e-6)
    return s / n


# =============================
# 2) Dataset LSA‑T Multi‑stream
# =============================

@dataclass
class SampleItem:
    face: torch.Tensor           # [T, 3, 224, 224]
    hand_l: torch.Tensor         # [T, 3, 224, 224]
    hand_r: torch.Tensor         # [T, 3, 224, 224]
    pose: torch.Tensor           # [T, 3*Lkp]
    pad_mask: torch.Tensor       # [T]
    miss_mask_hl: torch.Tensor   # [T] 1 válido, 0 faltante
    miss_mask_hr: torch.Tensor   # [T]
    text: str
    video_id: str


class LsaTMultiStream(Dataset):
    """Asume estructura de carpetas prepoblada con extract_rois_v2.py
    face/hand_l/hand_r: <video>_f%06d.jpg, pose/<video>.npz, y subs.csv (video_id;texto)
    """
    def __init__(self,
                 face_dir: str,
                 hand_l_dir: str,
                 hand_r_dir: str,
                 pose_dir: str,
                 csv_path: str,
                 index_csv: str,
                 T: int = 128,
                 img_size: int = 224,
                 lkp_count: int = 13,
                 min_conf: float = 0.25,
                 flip_prob: float = 0.2):
        from PIL import Image
        import pandas as pd
        self.Image = Image
        self.pd = pd
        self.face_dir = face_dir
        self.hand_l_dir = hand_l_dir
        self.hand_r_dir = hand_r_dir
        self.pose_dir = pose_dir
        self.img_size = img_size
        self.T = T
        self.lkp_count = lkp_count
        self.min_conf = min_conf
        self.flip_prob = flip_prob

        # cargar csv de textos
        df = pd.read_csv(csv_path, sep=';')
        df.columns = [c.strip().lower() for c in df.columns]
        assert 'video_id' in df.columns and 'texto' in df.columns
        # cargar split index
        idx = pd.read_csv(index_csv)
        idx.columns = [c.strip().lower() for c in idx.columns]
        assert 'video_id' in idx.columns
        self.df = df.merge(idx[['video_id']], on='video_id', how='inner')
        self.ids = self.df['video_id'].tolist()
        self.texts = dict(zip(df['video_id'], df['texto']))

        # transforms simples (sin libs externas):
        import numpy as np
        self.np = np

    def __len__(self):
        return len(self.ids)

    def _read_image(self, path: str):
        # lectura robusta con PIL → tensor CHW normalizado a [0,1]
        import numpy as np
        from PIL import Image
        img = Image.open(path).convert('RGB').resize((self.img_size, self.img_size))
        arr = np.asarray(img, dtype='float32') / 255.0
        # HWC→CHW
        arr = arr.transpose(2,0,1)
        return torch.from_numpy(arr)

    def _list_frames(self, base_dir: str, vid: str):
        # lista de archivos ordenados por índice f%06d
        files = []
        if not os.path.isdir(base_dir):
            return files
        prefix = f"{vid}_f"
        for name in sorted(os.listdir(base_dir)):
            if name.startswith(prefix) and name.endswith('.jpg'):
                files.append(os.path.join(base_dir, name))
        return files

    def _sample_indices(self, T0: int) -> List[int]:
        if T0 <= 0:
            return [0]*self.T
        if T0 >= self.T:
            stride = math.ceil(T0 / self.T)
            offset = random.randint(0, max(0, stride-1))
            idxs = list(range(offset, min(offset + stride*self.T, T0), stride))
            # pad si falta
            while len(idxs) < self.T:
                idxs.append(idxs[-1])
            return idxs[:self.T]
        else:
            # repetir últimos
            idxs = list(range(T0))
            while len(idxs) < self.T:
                idxs.append(idxs[-1])
            return idxs[:self.T]

    def __getitem__(self, i: int) -> SampleItem:
        vid = self.ids[i]
        txt = str(self.texts.get(vid, "")).strip()
        face_frames = self._list_frames(self.face_dir, vid)
        hl_frames = self._list_frames(self.hand_l_dir, vid)
        hr_frames = self._list_frames(self.hand_r_dir, vid)
        T0 = max(len(face_frames), len(hl_frames), len(hr_frames))
        idxs = self._sample_indices(T0)

        # lee pose
        pose_path = os.path.join(self.pose_dir, f"{vid}.npz")
        if os.path.exists(pose_path):
            pose_npz = self.np.load(pose_path)
            pose = pose_npz['pose']  # [T0, 3*Lkp]
            if pose.shape[0] == 0:
                pose = self.np.zeros((1, 3*self.lkp_count), dtype='float32')
        else:
            pose = self.np.zeros((1, 3*self.lkp_count), dtype='float32')

        # muestreo por índices
        def safe_get(frames, j):
            if len(frames) == 0:
                return None
            return frames[min(j, len(frames)-1)]

        face_list, hl_list, hr_list = [], [], []
        miss_hl, miss_hr = [], []
        for j in idxs:
            fp = safe_get(face_frames, j)
            lp = safe_get(hl_frames, j)
            rp = safe_get(hr_frames, j)
            face_list.append(self._read_image(fp) if fp else torch.zeros(3, self.img_size, self.img_size))
            if lp:
                hl_list.append(self._read_image(lp))
                miss_hl.append(1)
            else:
                hl_list.append(torch.zeros(3, self.img_size, self.img_size))
                miss_hl.append(0)
            if rp:
                hr_list.append(self._read_image(rp))
                miss_hr.append(1)
            else:
                hr_list.append(torch.zeros(3, self.img_size, self.img_size))
                miss_hr.append(0)

        face = torch.stack(face_list, dim=0)
        hand_l = torch.stack(hl_list, dim=0)
        hand_r = torch.stack(hr_list, dim=0)

        # pose muestreada
        T0p = pose.shape[0]
        if T0p <= 0:
            pose_s = self.np.zeros((self.T, 3*self.lkp_count), dtype='float32')
        else:
            idxs_p = self._sample_indices(T0p)
            pose_s = pose[idxs_p]
        pose_t = torch.from_numpy(pose_s.astype('float32'))

        pad_mask = torch.ones(self.T, dtype=torch.bool)  # no pad aquí (ya forzamos T)
        miss_mask_hl = torch.tensor(miss_hl, dtype=torch.bool)
        miss_mask_hr = torch.tensor(miss_hr, dtype=torch.bool)

        # normalización ImageNet
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)
        std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)
        face = (face - mean) / std
        hand_l = (hand_l - mean) / std
        hand_r = (hand_r - mean) / std

        return SampleItem(face, hand_l, hand_r, pose_t, pad_mask, miss_mask_hl, miss_mask_hr, txt, vid)


def collate_fn(batch: List[SampleItem]) -> Dict[str, Any]:
    B = len(batch)
    T = batch[0].face.shape[0]
    H = batch[0].face.shape[-1]
    W = H
    def stack_attr(attr):
        return torch.stack([getattr(b, attr) for b in batch], dim=0)
    out = {
        'face': stack_attr('face'),        # [B,T,3,224,224]
        'hand_l': stack_attr('hand_l'),
        'hand_r': stack_attr('hand_r'),
        'pose': stack_attr('pose'),        # [B,T,3*Lkp]
        'pad_mask': stack_attr('pad_mask'),
        'miss_mask_hl': stack_attr('miss_mask_hl'),
        'miss_mask_hr': stack_attr('miss_mask_hr'),
        'texts': [b.text for b in batch],
        'video_ids': [b.video_id for b in batch]
    }
    return out


# =============================
# 3) Backbones (placeholders)
# =============================

class ViTSmallPatch16(nn.Module):
    """Placeholder de ViT-S/16. En producción, reemplazar por DINOv2‑S cargado.
    Devuelve embedding de 384 canales por frame.
    """
    def __init__(self, embed_dim=384):
        super().__init__()
        self.conv = nn.Conv2d(3, embed_dim, kernel_size=16, stride=16)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    def forward(self, x):  # [B*T,3,224,224]
        x = self.conv(x)
        x = self.pool(x)
        x = x.flatten(1)
        return x  # [B*T,384]


# =============================
# 4) Proyectores y fusión
# =============================

class StreamProjector(nn.Module):
    def __init__(self, in_dim, out_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, out_dim),
            nn.GELU(),
            nn.LayerNorm(out_dim)
        )
    def forward(self, x):
        return self.net(x)


class FuseConcatLinear(nn.Module):
    def __init__(self, d_in_concat=1024, d_model=512, dropout=0.1):
        super().__init__()
        self.lin = nn.Linear(d_in_concat, d_model)
        self.ln = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)
    def forward(self, zF, zHL, zHR, zP):
        z = torch.cat([zF, zHL, zHR, zP], dim=-1)
        z = self.lin(z)
        z = F.gelu(z)
        z = self.ln(z)
        z = self.drop(z)
        return z  # [B,T,D]


class PositionalEncodingLearned(nn.Module):
    def __init__(self, d_model=512, max_len=2048):
        super().__init__()
        self.pe = nn.Parameter(torch.zeros(1, max_len, d_model))
        nn.init.normal_(self.pe, std=0.02)
    def forward(self, x):
        T = x.size(1)
        return x + self.pe[:, :T, :]


# =============================
# 5) Encoder temporal + Decoder
# =============================

class TemporalEncoder(nn.Module):
    def __init__(self, d_model=512, nhead=8, nlayers=6, dim_feedforward=2048, dropout=0.1):
        super().__init__()
        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,
                                               dim_feedforward=dim_feedforward,
                                               dropout=dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)
    def forward(self, z, src_key_padding_mask=None):
        return self.encoder(z, src_key_padding_mask=src_key_padding_mask)


class TextDecoderStub(nn.Module):
    """Stub: reemplazar por mBART‑50 de HuggingFace en producción.
    Aquí solo proyectamos a vocab para que el script sea ejecutable.
    """
    def __init__(self, d_model=512, vocab_size=32000):
        super().__init__()
        self.lm_head = nn.Linear(d_model, vocab_size)
    def forward(self, enc_out, target_tokens=None):
        # para el stub, tomamos el promedio temporal y proyectamos
        h = enc_out.mean(dim=1)
        logits = self.lm_head(h)
        return logits


class MultiStreamSLT(nn.Module):
    def __init__(self, d_model=512, proj_dim=256, nhead=8, nlayers=6, vocab_size=32000, lkp_count=13):
        super().__init__()
        # Backbones (placeholder ViT). Reemplazar por DINOv2 face/hands en producción
        self.enc_face = ViTSmallPatch16(384)
        self.enc_hands = ViTSmallPatch16(384)

        # Proyectores
        self.proj_F  = StreamProjector(384, proj_dim)
        self.proj_HL = StreamProjector(384, proj_dim)
        self.proj_HR = StreamProjector(384, proj_dim)
        self.proj_P1 = nn.Linear(3*lkp_count, 128)
        self.proj_P2 = StreamProjector(128, proj_dim)

        # Fusión y temporal
        self.fuse = FuseConcatLinear(4*proj_dim, d_model)
        self.posenc = PositionalEncodingLearned(d_model)
        self.temporal = TemporalEncoder(d_model, nhead, nlayers)

        # Decoder textual (stub)
        self.decoder = TextDecoderStub(d_model, vocab_size)

    def forward(self, face, hand_l, hand_r, pose, pad_mask=None, miss_mask_hl=None, miss_mask_hr=None):
        B, T = face.shape[:2]
        # aplastar batch temporal para backbones
        def enc_vit(vit, x):
            x = x.reshape(B*T, 3, 224, 224)
            y = vit(x)            # [B*T,384]
            return y.reshape(B, T, -1)

        f = enc_vit(self.enc_face, face)
        hl = enc_vit(self.enc_hands, hand_l)
        hr = enc_vit(self.enc_hands, hand_r)

        zF  = self.proj_F(f)
        zHL = self.proj_HL(hl)
        zHR = self.proj_HR(hr)
        zP  = self.proj_P2(F.gelu(self.proj_P1(pose)))

        z = self.fuse(zF, zHL, zHR, zP)
        z = self.posenc(z)
        # pad_mask True = válido? En Transformer se espera True=pad. Convertimos:
        src_key_padding_mask = None
        if pad_mask is not None:
            src_key_padding_mask = ~pad_mask  # True donde hay pad
        z = self.temporal(z, src_key_padding_mask)

        logits = self.decoder(z)
        return logits  # [B, vocab]


# =============================
# 6) Entrenamiento (loop básico)
# =============================

def train_epoch(model, loader, optimizer, scaler=None, device='cuda'):
    model.train()
    total = 0.0
    for batch in loader:
        face = batch['face'].to(device)
        hand_l = batch['hand_l'].to(device)
        hand_r = batch['hand_r'].to(device)
        pose = batch['pose'].to(device)
        pad_mask = batch['pad_mask'].to(device)
        # Texto no tokenizado en este stub; generamos etiquetas sintéticas
        # En producción: usar tokenizer mBART y teacher-forcing
        targets = torch.randint(0, 32000, (face.size(0),), device=device)

        optimizer.zero_grad(set_to_none=True)
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(face, hand_l, hand_r, pose, pad_mask)
                loss = F.cross_entropy(logits, targets, label_smoothing=0.1)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            logits = model(face, hand_l, hand_r, pose, pad_mask)
            loss = F.cross_entropy(logits, targets, label_smoothing=0.1)
            loss.backward()
            optimizer.step()
        total += loss.item() * face.size(0)
    return total / len(loader.dataset)


def eval_epoch(model, loader, device='cuda'):
    model.eval()
    total = 0.0
    with torch.no_grad():
        for batch in loader:
            face = batch['face'].to(device)
            hand_l = batch['hand_l'].to(device)
            hand_r = batch['hand_r'].to(device)
            pose = batch['pose'].to(device)
            pad_mask = batch['pad_mask'].to(device)
            targets = torch.randint(0, 32000, (face.size(0),), device=device)
            logits = model(face, hand_l, hand_r, pose, pad_mask)
            loss = F.cross_entropy(logits, targets, label_smoothing=0.1)
            total += loss.item() * face.size(0)
    return total / len(loader.dataset)


# =============================
# 7) Punto de entrada de ejemplo
# =============================

if __name__ == '__main__':
    set_seed(123)

    # RUTAS — AJUSTAR A TU ENTORNO
    FACE_DIR = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\face"
    HAND_L_DIR = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\hand_l"
    HAND_R_DIR = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\hand_r"
    POSE_DIR = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\pose"
    SUBS = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\lsa_t\subs.csv"
    TRAIN_IDX = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\lsa_t\index\train.csv"
    VAL_IDX = r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\lsa_t\index\val.csv"

    T = 64
    BATCH = 2

    train_set = LsaTMultiStream(FACE_DIR, HAND_L_DIR, HAND_R_DIR, POSE_DIR, SUBS, TRAIN_IDX, T=T)
    val_set   = LsaTMultiStream(FACE_DIR, HAND_L_DIR, HAND_R_DIR, POSE_DIR, SUBS, VAL_IDX, T=T)

    train_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True, num_workers=0, collate_fn=collate_fn)
    val_loader   = DataLoader(val_set, batch_size=BATCH, shuffle=False, num_workers=0, collate_fn=collate_fn)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = MultiStreamSLT(d_model=512, proj_dim=256, nhead=8, nlayers=4, vocab_size=32000, lkp_count=13).to(device)

    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)
    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))

    for epoch in range(2):  # demo corto; en producción 40 epochs con curriculum T
        tr = train_epoch(model, train_loader, opt, scaler, device)
        va = eval_epoch(model, val_loader, device)
        print({"epoch": epoch+1, "train_loss": tr, "val_loss": va})

    print("Demo OK — Reemplazar stub de decoder por mBART y backbones por DINOv2.")

# ================================================
# file: tools/extract_rois_v2.py
# ROI extractor (cara, mano izq., mano der.) + pose superior
# Requiere: mediapipe, opencv-python, numpy, pillow
# ================================================

import os
import cv2
import math
import json
import numpy as np
from pathlib import Path
from typing import Tuple, Dict

try:
    import mediapipe as mp
except Exception as e:
    mp = None
    print("[WARN] MediaPipe no disponible. Instala mediapipe para extraer ROIs.")


def ensure_dir(p: str):
    Path(p).mkdir(parents=True, exist_ok=True)


def expand_clamp_bbox(x, y, w, h, scale, W, H):
    cx, cy = x + w/2, y + h/2
    w2, h2 = w*scale, h*scale
    x1 = max(0, int(cx - w2/2))
    y1 = max(0, int(cy - h2/2))
    x2 = min(W, int(cx + w2/2))
    y2 = min(H, int(cy + h2/2))
    return x1, y1, x2-x1, y2-y1


def crop_square(img, x, y, w, h, out_size=224):
    H, W = img.shape[:2]
    side = max(w, h)
    cx, cy = x + w//2, y + h//2
    x1 = max(0, cx - side//2)
    y1 = max(0, cy - side//2)
    x2 = min(W, x1 + side)
    y2 = min(H, y1 + side)
    crop = img[y1:y2, x1:x2]
    crop = cv2.resize(crop, (out_size, out_size), interpolation=cv2.INTER_LINEAR)
    return crop


def blur_face_preserve_eyes_mouth(img, face_landmarks):
    # opcional: desenfoca salvo ojos y boca usando máscaras simples
    if face_landmarks is None:
        return img
    H, W = img.shape[:2]
    mask = np.zeros((H, W), dtype=np.uint8)
    # índices aproximados de malla facial (ojo-izq 33..133, ojo-der 263..362, labios 0..17 aprox) — placeholders
    keep_idxs = list(range(0, 18)) + list(range(33, 134)) + list(range(263, 363))
    pts = []
    for k, lm in enumerate(face_landmarks.landmark):
        if k in keep_idxs:
            pts.append((int(lm.x*W), int(lm.y*H)))
    if len(pts) > 0:
        for (x,y) in pts:
            cv2.circle(mask, (x,y), 6, 255, -1)
    blurred = cv2.GaussianBlur(img, (31,31), 0)
    mask3 = cv2.merge([mask,mask,mask])
    inv = 255 - mask3
    out = cv2.bitwise_and(blurred, inv) + cv2.bitwise_and(img, mask3)
    return out


def process_video(vid_path: str, out_dirs: Dict[str, str], pose_dir: str, fps_target=25, face_blur=False):
    assert mp is not None, "MediaPipe requerido"
    mp_face = mp.solutions.face_mesh
    mp_hands = mp.solutions.hands
    mp_pose = mp.solutions.pose

    cap = cv2.VideoCapture(vid_path)
    if not cap.isOpened():
        print(f"[ERR] No se pudo abrir {vid_path}")
        return False

    basename = Path(vid_path).stem
    face_out = out_dirs['face']
    hl_out = out_dirs['hand_l']
    hr_out = out_dirs['hand_r']

    ensure_dir(face_out); ensure_dir(hl_out); ensure_dir(hr_out); ensure_dir(pose_dir)

    fps = cap.get(cv2.CAP_PROP_FPS) or fps_target
    stride = max(1, int(round(fps / fps_target)))

    # MediaPipe contexts
    with mp_face.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True) as fmesh, \
         mp_hands.Hands(static_image_mode=False, max_num_hands=2) as hands, \
         mp_pose.Pose(static_image_mode=False, model_complexity=1) as pose:

        idx = 0
        t = 0
        pose_list = []
        while True:
            ok, frame = cap.read()
            if not ok: break
            if idx % stride != 0:
                idx += 1
                continue
            H, W = frame.shape[:2]
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            f_res = fmesh.process(rgb)
            h_res = hands.process(rgb)
            p_res = pose.process(rgb)

            # FACE bbox simple de landmarks
            face_crop = np.zeros((224,224,3), dtype=np.uint8)
            if f_res.multi_face_landmarks:
                flm = f_res.multi_face_landmarks[0]
                xs = [int(l.x*W) for l in flm.landmark]
                ys = [int(l.y*H) for l in flm.landmark]
                x1, y1, x2, y2 = max(0,min(xs)), max(0,min(ys)), min(W,max(xs)), min(H,max(ys))
                x,y,w,h = expand_clamp_bbox(x1, y1, x2-x1, y2-y1, 1.2, W, H)
                fc = frame[y:y+h, x:x+w].copy()
                if face_blur:
                    fc = blur_face_preserve_eyes_mouth(fc, flm)
                face_crop = crop_square(fc, 0, 0, w, h, 224)
            cv2.imwrite(os.path.join(face_out, f"{basename}_f{t:06d}.jpg"), face_crop)

            # HANDS — asignación izq/der por coordenada x respecto al centro
            hl_crop = np.zeros((224,224,3), dtype=np.uint8)
            hr_crop = np.zeros((224,224,3), dtype=np.uint8)
            if h_res.multi_hand_landmarks:
                hands_info = []
                for hlm, handed in zip(h_res.multi_hand_landmarks, h_res.multi_handedness):
                    xs = [int(l.x*W) for l in hlm.landmark]
                    ys = [int(l.y*H) for l in hlm.landmark]
                    x1,y1,x2,y2 = max(0,min(xs)), max(0,min(ys)), min(W,max(xs)), min(H,max(ys))
                    hands_info.append(((x1,y1,x2,y2), hlm, handed.classification[0].label))
                # ordenar por etiqueta
                for (x1,y1,x2,y2), hlm, label in hands_info:
                    x,y,w,h = expand_clamp_bbox(x1, y1, x2-x1, y2-y1, 1.2, W, H)
                    hc = crop_square(frame, x, y, w, h, 224)
                    if label.lower().startswith('left'):
                        hl_crop = hc
                    else:
                        hr_crop = hc
            cv2.imwrite(os.path.join(hl_out, f"{basename}_f{t:06d}.jpg"), hl_crop)
            cv2.imwrite(os.path.join(hr_out, f"{basename}_f{t:06d}.jpg"), hr_crop)

            # POSE superior (13–17 puntos)
            lmk = np.zeros((17,3), dtype=np.float32)
            if p_res.pose_landmarks:
                for k, lm in enumerate(p_res.pose_landmarks.landmark[:17]):
                    lmk[k,0] = lm.x
                    lmk[k,1] = lm.y
                    lmk[k,2] = lm.visibility
            pose_list.append(lmk.reshape(-1))

            t += 1
            idx += 1

    cap.release()
    pose_arr = np.array(pose_list, dtype=np.float32)
    np.savez(os.path.join(pose_dir, f"{basename}.npz"), pose=pose_arr)
    return True


def run_bulk(videos_dir: str, out_root: str, fps_target=25, face_blur=False):
    vd = Path(videos_dir)
    out_dirs = {
        'face': os.path.join(out_root, 'face'),
        'hand_l': os.path.join(out_root, 'hand_l'),
        'hand_r': os.path.join(out_root, 'hand_r'),
    }
    pose_dir = os.path.join(out_root, 'pose')
    ensure_dir(pose_dir)

    for p in vd.glob('*.mp4'):
        print(f"Procesando {p.name}")
        process_video(str(p), out_dirs, pose_dir, fps_target, face_blur)


if __name__ == '__main__':
    # Ajusta rutas
    run_bulk(
        videos_dir=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\lsa_t\videos",
        out_root=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois",
        fps_target=25,
        face_blur=False
    )


# ================================================
# file: tools/pretrain_dino_face.py (stub ejecutable)
# ================================================

# En producción, cambia este stub por tu pipeline DINOv2. Aquí guardamos un encoder ViT-S pequeño
# entrenado de forma auto-supervisada mínima (placeholder) para poder correr de punta a punta.

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from pathlib import Path

class ImageFolderFlat(Dataset):
    def __init__(self, root: str, img_size=224):
        from PIL import Image
        import numpy as np
        self.Image = Image; self.np=np
        self.files = [str(p) for p in Path(root).glob('*.jpg')]
        self.img_size = img_size
    def __len__(self):
        return len(self.files)
    def __getitem__(self, i):
        from PIL import Image
        import numpy as np
        p = self.files[i]
        arr = np.asarray(Image.open(p).convert('RGB').resize((self.img_size,self.img_size)), dtype='float32')/255.
        arr = arr.transpose(2,0,1)
        x = torch.from_numpy(arr)
        return x

class TinyViT(nn.Module):
    def __init__(self, dim=384):
        super().__init__()
        self.conv = nn.Conv2d(3, dim, 16, 16)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    def forward(self, x):
        x = self.conv(x); x=self.pool(x); return x.flatten(1)


def train_stub(img_dir: str, out_dir: str, epochs=2, batch=256, lr=1e-3):
    ds = ImageFolderFlat(img_dir)
    dl = DataLoader(ds, batch_size=batch, shuffle=True, num_workers=0)
    enc = TinyViT().cuda() if torch.cuda.is_available() else TinyViT()
    opt = torch.optim.AdamW(enc.parameters(), lr=lr)
    for ep in range(epochs):
        enc.train(); total=0
        for x in dl:
            x = x.cuda() if torch.cuda.is_available() else x
            h = enc(x)
            # pérdida dummy: empujar a norma unitaria (simula objetivo auto-sup.)
            loss = (h.pow(2).sum(dim=1).sqrt() - 1).abs().mean()
            opt.zero_grad(); loss.backward(); opt.step(); total += loss.item()*x.size(0)
        print({"epoch": ep+1, "loss": total/len(ds)})
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    torch.save(enc.state_dict(), str(Path(out_dir)/'encoder.pth'))

if __name__=='__main__':
    train_stub(
        img_dir=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\face",
        out_dir=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\work_dirs\dino_face_v1"
    )


# ================================================
# file: tools/pretrain_dino_hands.py (stub ejecutable)
# ================================================

import os

def train_stub_hands(dir_l: str, dir_r: str, out_dir: str):
    # Reutiliza el stub anterior concatenando ambas carpetas
    import shutil, tempfile
    tmp = tempfile.mkdtemp()
    for d in [dir_l, dir_r]:
        for f in os.listdir(d):
            if f.endswith('.jpg'):
                src = os.path.join(d,f)
                dst = os.path.join(tmp,f"{os.path.basename(d)}_{f}")
                shutil.copyfile(src,dst)
    from pretrain_dino_face import train_stub
    train_stub(tmp, out_dir, epochs=2)

if __name__=='__main__':
    train_stub_hands(
        dir_l=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\hand_l",
        dir_r=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\data\rois\hand_r",
        out_dir=r"G:\Apps\SENAR\v5 - MSKA\MSKA-main\work_dirs\dino_hands_v1"
    )


# ================================================
# file: tools/train_slt_multistream_v9.py
# Entrenador completo (usa el modelo del main de este archivo)
# ================================================

import argparse
import torch
from torch.utils.data import DataLoader

from __main__ import LsaTMultiStream, collate_fn, MultiStreamSLT, set_seed


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--face_dir', required=True)
    ap.add_argument('--hand_l_dir', required=True)
    ap.add_argument('--hand_r_dir', required=True)
    ap.add_argument('--pose_dir', required=True)
    ap.add_argument('--csv', required=True)
    ap.add_argument('--splits_dir', required=True)
    ap.add_argument('--work_dir', required=True)
    ap.add_argument('--epochs', type=int, default=40)
    ap.add_argument('--batch_size', type=int, default=4)
    ap.add_argument('--grad_accum', type=int, default=4)
    ap.add_argument('--T', type=int, default=128)
    ap.add_argument('--device', default='cuda')
    args = ap.parse_args()

    set_seed(123)
    train_set = LsaTMultiStream(args.face_dir, args.hand_l_dir, args.hand_r_dir, args.pose_dir,
                                args.csv, os.path.join(args.splits_dir,'train.csv'), T=args.T)
    val_set   = LsaTMultiStream(args.face_dir, args.hand_l_dir, args.hand_r_dir, args.pose_dir,
                                args.csv, os.path.join(args.splits_dir,'val.csv'), T=args.T)
    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)
    val_loader   = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)

    device = args.device if torch.cuda.is_available() else 'cpu'
    model = MultiStreamSLT(d_model=512, proj_dim=256, nhead=8, nlayers=6, vocab_size=32000, lkp_count=13).to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)
    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))

    best = 1e9
    for epoch in range(args.epochs):
        tr = train_epoch(model, train_loader, opt, scaler, device)
        va = eval_epoch(model, val_loader, device)
        print({"epoch": epoch+1, "train_loss": tr, "val_loss": va})
        if va < best:
            best = va
            os.makedirs(args.work_dir, exist_ok=True)
            torch.save(model.state_dict(), os.path.join(args.work_dir,'best.pth'))

if __name__=='__main__':
    import os
    from __main__ import train_epoch, eval_epoch
    main()


# ================================================
# file: tools/eval_slt_multistream_v9.py
# Eval: métricas textuales (stubs) + guardado de muestras
# ================================================

import argparse
import csv

from __main__ import LsaTMultiStream, collate_fn, MultiStreamSLT


def main_eval():
    ap = argparse.ArgumentParser()
    ap.add_argument('--checkpoint', required=True)
    ap.add_argument('--face_dir', required=True)
    ap.add_argument('--hand_l_dir', required=True)
    ap.add_argument('--hand_r_dir', required=True)
    ap.add_argument('--pose_dir', required=True)
    ap.add_argument('--csv', required=True)
    ap.add_argument('--splits_dir', required=True)
    ap.add_argument('--out_csv', required=True)
    args = ap.parse_args()

    ds = LsaTMultiStream(args.face_dir, args.hand_l_dir, args.hand_r_dir, args.pose_dir,
                         args.csv, os.path.join(args.splits_dir,'test.csv'), T=128)
    dl = DataLoader(ds, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = MultiStreamSLT().to(device)
    model.load_state_dict(torch.load(args.checkpoint, map_location=device))
    model.eval()

    rows = []
    with torch.no_grad():
        for batch in dl:
            face = batch['face'].to(device)
            hand_l = batch['hand_l'].to(device)
            hand_r = batch['hand_r'].to(device)
            pose = batch['pose'].to(device)
            pad_mask = batch['pad_mask'].to(device)
            logits = model(face, hand_l, hand_r, pose, pad_mask)
            preds = logits.argmax(dim=-1).tolist()
            for vid, gold, pred in zip(batch['video_ids'], batch['texts'], preds):
                rows.append([vid, gold, str(pred)])

    with open(args.out_csv,'w',newline='',encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(['video_id','gold_text','pred_stub'])
        w.writerows(rows)

if __name__=='__main__':
    import os
    import torch
    from torch.utils.data import DataLoader
    main_eval()


# ================================================
# file: tools/export_onnx_encoder_v9.py (stub)
# ================================================

import argparse
import torch
from __main__ import MultiStreamSLT

def main_export():
    ap = argparse.ArgumentParser()
    ap.add_argument('--checkpoint', required=True)
    ap.add_argument('--out', required=True)
    args = ap.parse_args()

    device = 'cpu'
    model = MultiStreamSLT().to(device)
    model.load_state_dict(torch.load(args.checkpoint, map_location=device))
    model.eval()

    # Encoder completo hasta temporal (salida B×D). Para el stub exportamos forward completo
    dummy_face = torch.randn(1,64,3,224,224)
    dummy_hand = torch.randn(1,64,3,224,224)
    dummy_pose = torch.randn(1,64,39)  # 13 lmk * 3
    dummy_mask = torch.ones(1,64,dtype=torch.bool)

    torch.onnx.export(model,
                      (dummy_face, dummy_hand, dummy_hand, dummy_pose, dummy_mask, None, None),
                      args.out,
                      input_names=['face','hand_l','hand_r','pose','pad_mask','miss_mask_hl','miss_mask_hr'],
                      output_names=['logits'],
                      dynamic_axes={'face':{1:'T'}, 'hand_l':{1:'T'}, 'hand_r':{1:'T'}, 'pose':{1:'T'}, 'pad_mask':{1:'T'}},
                      opset_version=17)

if __name__=='__main__':
    main_export()


# ================================================
# file: tools/demo_realtime_multistream.py (esqueleto)
# ================================================

# Usa webcam, MediaPipe para ROIs en vivo, pasa por el modelo cargado y muestra texto (stub)

